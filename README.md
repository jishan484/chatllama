# ChatLLaMA <img width="31" height="33" alt="image" src="https://github.com/user-attachments/assets/94da76b2-eb2d-41c4-a9fe-2d8f23070482" />

> <img width="574" height="268" alt="image" src="https://github.com/user-attachments/assets/60fb7751-f3c4-4661-87cb-e056eaa02349" />

*UI and local proxy for Ollama â€” chat with LLaMA and other models from any device on your network.*

ChatLLaMA is a lightweight web interface and proxy server that allows you to chat with Ollama models running on your host machine â€” from mobile, tablet, or other computers. It exposes a simple UI and REST endpoint so you donâ€™t have to stay on the same PC.

## âœ¨ Features
- ğŸ”„ Model Selector â€“ switch between installed models (e.g. llama3, mistral, gemma, etc.)
- ğŸ’¬ Chat Interface â€“ stream responses from models in real-time
- ğŸŒˆ Output Highlighting â€“ style model responses for readability
- ğŸ§  Code Highlighting â€“ syntax-aware blocks for code in model replies
- ğŸ“‹ Copy Code Button â€“ quickly copy code snippets with one click
- âš¡ Keyboard Shortcuts â€“ navigate UI and send messages efficiently
- ğŸ§© Proxy API â€“ exposes Ollama's /api/chat endpoint over your LAN
- ğŸ“ Chat History â€“ persist chat sessions for seamless follow-up conversations

## ğŸš€ Getting Started
- Required `pyton3` (no external dependency required)
- Ollama installed and running on your host
- Models pulled via ollama pull llama3 (or other)
1. Run `python3 host.py`
2. Then open in browser: <br>
ğŸ“± `http://<host-ip>:8000` (accessible from mobile or other devices on same/any network)

âš™ï¸ Configuration
- You need to modify host.py to change the port

## ğŸ¨ UI
- Resizable input box with multiline support
- Keyboard shortcuts:
  * Enter to send
  * Ctrl+D to start New Chat
  * Ctrl+S: Stop output
- Automatic scroll to latest response
- Persistent chat history

##  ğŸ” Security Notes
This app exposes your Ollama instance to the network.
To restrict access:
- Use firewall/IP filtering
- Add authentication (coming soon)

## ğŸ› ï¸ TODO / Roadmap
 * Multi-user sessions
 * Auth layer for public deployment
 * Prompt presets
 * Voice input & output
 * Mobile PWA install

# ğŸ§  Why?
Ollama is powerful â€” but it's limited to your local machine ğŸ˜­.
ChatLLaMA lets you access local models like llama3 from any device on your network ğŸ¤©, with a lightweight and user-friendly web interface.
No need to install heavy apps or frameworks â€” just run this simple web app and start chatting. ğŸ˜

## ğŸ“„ License
MIT License â€” feel free to fork, contribute, and make it better!
